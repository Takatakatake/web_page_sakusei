{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0a2920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('kulp', ['有罪', 40000]), ('alkov', ['QQQ&#97;&#108;&#107;&#111;&#118;', 30001]), ('dosier', ['文件', 60000]), ('interlingvistik', ['QQQ&#105;&#110;&#116;&#101;&#114;&#108;&#105;&#110;&#103;&#118;&#105;&#115;&#116;&#105;&#107;', 150000]), ('kaj', ['QQQ&#107;&#97;&#106;', 30000]), ('cxar', ['なので', 40001]), ('brasil', ['QQQ&#98;&#114;&#97;&#115;&#105;&#108;', 60001]), ('rektangul', ['长方形', 90001]), ('malm', ['malm', 20001]), ('avis', ['abes', 10001]), ('ari', ['ari', 20001]), ('din', ['din', 20000]), ('devis', ['必is', 40001])])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "#コード上でもテキストファイル内(後から加える語根リスト.txt)でも後から加える語根のリストを編集できるようにした。(優先順位も決められる)\n",
    "#コード上で後から加える語根のリストを編集(こちらがテキストファイル内(後から加える語根リスト.txt)で後から加える語根のリストよりも優先される)\n",
    "add_vocab = OrderedDict([\n",
    "    (\"kulp\", [\"有罪\", 40000]),#数字(デフォルトは語根の文字数*10000,変更可能)の大きい順に置換されていく。30001であれば3文字の語根(30000)よりも先に置換される。\n",
    "    (\"alkov\", [30001]),#訳語がないものは語根のままで後から置換されないようにするため、unicode化された訳語がリストの頭に追加される。\n",
    "    #すなわち、\"alkov\"は一旦'QQQ&#97;&#108;&#107;&#111;&#118;'に変換され、その後\"alkov\"に戻される。(この際、元の(変換前の)語根が小文字か大文字か頭だけ大文字かを確認し、変換後もそれに合わせるようにできている。)\n",
    "    (\"dosier\", [\"文件\"]),##数字がないものは、語根の文字数*10000が自動的にリストに追加される。\n",
    "    (\"interlingvistik\",[]),##数字も訳語もないものはunicode化された訳語と辞書のキーの長さ*10000が自動的にリストに追加される。\n",
    "    (\"kaj\",[])])\n",
    "\n",
    "#テキストファイル内(後から加える語根リスト.txt)で後から加える語根のリストを編集\n",
    "with open('後から加える語根リスト(優先順位も決められる).txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.rstrip()\n",
    "        data = line.split(',')\n",
    "        if data[0] in add_vocab or data[0]=='':##\"コード上で後から加える語根のリスト\"を優先\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                add_vocab[data[0]]=[data[1],int(data[2])]\n",
    "            except:\n",
    "                try:\n",
    "                    add_vocab[data[0]]=[int(data[1])]\n",
    "                except:\n",
    "                    try:\n",
    "                        add_vocab[data[0]]=[data[1]]\n",
    "                    except:\n",
    "                        add_vocab[data[0]]=[]\n",
    "                \n",
    "                \n",
    "for key, value in add_vocab.items():\n",
    "    if len(value) == 1:\n",
    "        if isinstance(value[0],str):##リストに訳語が入っている場合はその後にキーである語根の文字数*10000の数字を追加する。\n",
    "            char_count = len(key) * 10000\n",
    "            add_vocab[key].append(char_count)\n",
    "        elif  isinstance(value[0],(int, float)):##リストに数字が入っている場合は訳語を前に追加する。(訳語はunicode化)\n",
    "            unicode_ka ='QQQ'+''.join([\"&#\"+str(ord(c))+\";\" for c in key])##\"Q\"にしたのはエスペラントには\"Q\"がなく、別の語根に食われて\n",
    "            ## しまう心配がないから。一部でも食われると、unicode化した状態からもとに戻れなくなる。　また、変換時は、qqq,QQQ,Qqqに変えることによって小文字か大文字か頭だけ大文字かの情報を保存している。\n",
    "            add_vocab[key].insert(0,unicode_ka)\n",
    "\n",
    "    elif  len(value) == 0:##リストに何も入っていない(空)場合は、訳語も数字も追加する。\n",
    "        unicode_ka ='QQQ'+''.join([\"&#\"+str(ord(c))+\";\" for c in key])\n",
    "        char_count = len(key) * 10000\n",
    "\n",
    "        add_vocab[key].append(unicode_ka)\n",
    "        add_vocab[key].append(char_count)\n",
    "        \n",
    "print(add_vocab)##うまく行っているか確認\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23b52b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esperantoの語根と意味をロードする\n",
    "roots_dict = OrderedDict([])\n",
    "\n",
    "with open('全語根と一部の語根に対応する漢字のリスト.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if ',' in line:\n",
    "            root, meaning = line.split(',', 1)\n",
    "            roots_dict[root] = [meaning,len(root)*10000]##訳語と数字(語根の文字数*10000)を追加\n",
    "        else:\n",
    "            # 対応する漢字が存在しない場合の処理\n",
    "            unicode_str = 'QQQ' + ''.join([\"&#\"+str(ord(c))+\";\" for c in line])#対応する漢字が存在しない場合はそのままにして、あとから別の語根によって漢字化されてしまわないようにする。\n",
    "            roots_dict[line] = [unicode_str,len(line)*10000]#すなわち、一旦アルファベットをunicode化したものを対応させ、あとから元のアルファベットに戻すようにする。\n",
    "            \n",
    "for key, value in add_vocab.items():##後から加える語根のリストを追加 無論重複している語根は更新される。\n",
    "    roots_dict[key]=value\n",
    "    \n",
    "# 数字(デフォルトは語根の文字数*10000,変更可能)の順にソート\n",
    "sorted_items = sorted(roots_dict.items(), key=lambda x: -x[1][1])###数字の大きい順に並べる。数字の大きい順に語根を置換していく。\n",
    "sorted_vocab = OrderedDict(sorted_items)##ソートでリスト型になったのを再度OrderedDict型に戻す。\n",
    "# print(sorted_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##振り仮名(アルファベット)をつけるための処理 html形式でルビをふる。 \n",
    "import copy\n",
    "sorted_keys = list(sorted_vocab.keys())\n",
    "sorted_vocab_copy=copy.deepcopy(sorted_vocab)#コピーを作っておく。 deepcopyでないと浅いコピー(同期)になってしまう。unicode化したものを戻すときに用いる。\n",
    "for key in sorted_keys:\n",
    "    if \"&#\" not in sorted_vocab[key][0]:##[\"mal\",\"<qqq&#114;&#117;&#98;y>反<rt>qqq&#109;&#97;&#108;</rt>\"]\n",
    "        key_2=\"QQQ\"+''.join([\"&#\"+str(ord(c))+\";\" for c in key])#振り仮名が後で変換されてしまうことを防ぐため。\n",
    "        sorted_vocab[key][0]='<ruby>{}<rt>{}</rt></ruby>'.format(sorted_vocab[key][0],key_2)\n",
    "        \n",
    "for key in sorted_keys:##[\"mal\",\"qqq&#109;&#97;&#108;\"]\n",
    "    if \"&#\" not in sorted_vocab_copy[key][0]:\n",
    "        key_2=\"QQQ\"+''.join([\"&#\"+str(ord(c))+\";\" for c in key])#振り仮名が後で変換されてしまうことを防ぐため。\n",
    "        sorted_vocab_copy[key][0]=key_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# OrderedDictをJSONファイルに保存\n",
    "with open('sorted_vocab.json', 'w') as f:\n",
    "    json.dump(sorted_vocab, f, indent=4)\n",
    "with open('sorted_vocab_copy.json', 'w') as f:\n",
    "    json.dump(sorted_vocab_copy, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##もどす\n",
    "with open('sorted_vocab.json', 'r') as f:\n",
    "    sorted_vocab = json.load(f, object_pairs_hook=OrderedDict)\n",
    "with open('sorted_vocab_copy.json', 'r') as f:\n",
    "    sorted_vocab_copy = json.load(f, object_pairs_hook=OrderedDict)\n",
    "    \n",
    "sorted_keys = list(sorted_vocab.keys())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034f02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replacer(s, new_word):###置換する語根すべてについて小文字か大文字か頭だけ大文字かを確認し、変換後もそれに合わせるための関数。 \n",
    "    if s.islower():##それぞれqqq,QQQ,Qqqから始まるunicode化した状態に変化させることで、元の語根の大文字小文字の情報を保存している。　あくまで近似である。 (頭だけ大文字の場合:Qqq&#110;&#117;&#114;)\n",
    "        return new_word.lower()\n",
    "    elif s.isupper():\n",
    "        return new_word.upper()\n",
    "    elif s[0].isupper():\n",
    "        return new_word.capitalize()\n",
    "    else:\n",
    "        return new_word\n",
    "\n",
    "def multi_smart_replace(text, replacements):##replacerを使って大文字小文字の状態を保ったままの変換\n",
    "    # for old_word, new_word in sorted_replacements:\n",
    "    for old_word, new_word in replacements.items():\n",
    "        text = re.sub(re.escape(old_word), lambda match: replacer(match.group(0), new_word), text, flags=re.IGNORECASE)\n",
    "\n",
    "    # print(sorted_replacements)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multiple_replace(text, replace_dict):\n",
    "    # 各キーを正規表現のパターンとして結合する\n",
    "    pattern = re.compile(\"|\".join([re.escape(k) for k in replace_dict.keys()]), re.M)\n",
    "\n",
    "    # 置換関数\n",
    "    def sub_func(match):\n",
    "        return replace_dict[match.group(0)]\n",
    "\n",
    "    # 置換を実行\n",
    "    return pattern.sub(sub_func, text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95709ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"sorted_vocab_copy.txt\", \"w\", encoding='utf-8') as file:\n",
    "#     for key, value in sorted_vocab_copy.items():\n",
    "#         file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "\n",
    "##sorted_vocab,sorted_vocab_copy,sorted_keysをファイルとして予め用意しておくと良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf657c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "esperanto_to_x = {\"ĉ\": \"cx\",\"ĝ\": \"gx\",\"ĥ\": \"hx\",\"ĵ\": \"jx\",\"ŝ\": \"sx\",\"ŭ\": \"ux\",\"Ĉ\": \"Cx\",\"Ĝ\": \"Gx\",\"Ĥ\": \"Hx\",\"Ĵ\": \"Jx\",\"Ŝ\": \"Sx\",\"Ŭ\": \"Ux\",\n",
    "\"c^\": \"cx\",\"g^\": \"gx\",\"h^\": \"hx\",\"j^\": \"jx\",\"s^\": \"sx\",\"u^\": \"ux\",\"C^\": \"Cx\",\"G^\": \"Gx\",\"H^\": \"Hx\",\"J^\": \"Jx\",\"S^\": \"Sx\",\"U^\": \"Ux\"}\n",
    "x_to_jijofu={'cx': 'ĉ', 'gx': 'ĝ', 'hx': 'ĥ', 'jx': 'ĵ', 'sx': 'ŝ', 'ux': 'ŭ', 'Cx': 'Ĉ',\n",
    "             'Gx': 'Ĝ', 'Hx': 'Ĥ', 'Jx': 'Ĵ', 'Sx': 'Ŝ', 'Ux': 'Ŭ'}\n",
    "x_to_hat={'cx': 'c^', 'gx': 'g^', 'hx': 'h^', 'jx': 'j^', 'sx': 's^', 'ux': 'u^', 'Cx': 'C^',\n",
    "          'Gx': 'G^', 'Hx': 'H^', 'Jx': 'J^', 'Sx': 'S^', 'Ux': 'U^'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20dc9175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the Esperanto text\n",
    "\n",
    "## 原文がX型か字上符型か^型かを確認(0はX型、1は字上符型、2は^型)\n",
    "letter_type=1\n",
    "\n",
    "with open('例文(input).txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text1 = multiple_replace(text, esperanto_to_x)\n",
    "\n",
    "replacements_1 = {key: sorted_vocab[key][0] for key in sorted_keys}\n",
    "text2 = multi_smart_replace(text1, replacements_1)\n",
    "\n",
    "# Build the unicode replacements dictionary\n",
    "replacements_2 = {sorted_vocab_copy[key][0]: key for key in sorted_keys if '&#' in sorted_vocab_copy[key][0]}\n",
    "text3 = multi_smart_replace(text2, replacements_2)\n",
    "\n",
    "\n",
    "if letter_type==1:\n",
    "    text4 = multiple_replace(text3, x_to_jijofu)\n",
    "elif letter_type==2:\n",
    "    text4 = multiple_replace(text3, x_to_hat)\n",
    "\n",
    "with open('語根に対応する漢字によって置換された文章(output).html', 'w', encoding='utf-8') as f:\n",
    "    f.write(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef864955",
   "metadata": {},
   "outputs": [],
   "source": [
    "##倍速並列処理\n",
    "import multiprocessing\n",
    "# process the Esperanto text\n",
    "\n",
    "## 原文がX型か字上符型か^型かを確認(0はX型、1は字上符型、2は^型)\n",
    "letter_type=1\n",
    "\n",
    "\n",
    "def process_segment(lines, letter_type, esperanto_to_x, x_to_jijofu, x_to_hat, replacements_1, replacements_2):\n",
    "    # 文字列のリストを結合してから置換処理を実行\n",
    "    segment = '\\n'.join(lines)\n",
    "    segment = multiple_replace(segment, esperanto_to_x)\n",
    "    segment = multi_smart_replace(segment, replacements_1)\n",
    "    segment = multi_smart_replace(segment, replacements_2)\n",
    "\n",
    "    if letter_type == 1:\n",
    "        segment = multiple_replace(segment, x_to_jijofu)\n",
    "    elif letter_type == 2:\n",
    "        segment = multiple_replace(segment, x_to_hat)\n",
    "\n",
    "    return segment\n",
    "\n",
    "def parallel_process(text, num_processes, letter_type, esperanto_to_x, x_to_jijofu, x_to_hat, replacements_1, replacements_2):\n",
    "    # テキストを行で分割\n",
    "    lines = text.split('\\n')\n",
    "    num_lines = len(lines)\n",
    "    lines_per_process = num_lines // num_processes\n",
    "\n",
    "    # 各プロセスに割り当てる行のリストを決定\n",
    "    ranges = [(i * lines_per_process, (i + 1) * lines_per_process) for i in range(num_processes)]\n",
    "    ranges[-1] = (ranges[-1][0], num_lines)  # 最後のプロセスが残り全てを処理\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        # 並列処理を実行\n",
    "        results = pool.starmap(process_segment, [(lines[start:end], letter_type, esperanto_to_x, x_to_jijofu, x_to_hat, replacements_1, replacements_2) for start, end in ranges])\n",
    "\n",
    "    # 結果を結合\n",
    "    return '\\n'.join(result for result in results)\n",
    "\n",
    "# テキストの読み込み\n",
    "with open('例文(input).txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()###文字数が増えれば増えるほど、処理時間がプロセス数(CPUの個数)に反比例するようになる。\n",
    "\n",
    "# 置換辞書を準備\n",
    "replacements_1 = {key: sorted_vocab[key][0] for key in sorted_keys}\n",
    "replacements_2 = {sorted_vocab_copy[key][0]: key for key in sorted_keys if '&#' in sorted_vocab_copy[key][0]}\n",
    "\n",
    "# 並列処理の実行\n",
    "processed_text = parallel_process(text, 8, letter_type, esperanto_to_x, x_to_jijofu, x_to_hat, replacements_1, replacements_2)\n",
    "\n",
    "# 結果をファイルに書き込み\n",
    "with open('語根に対応する漢字によって置換された文章(output).html', 'w', encoding='utf-8') as f:\n",
    "    f.write(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a00a7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##倍速並列処理 メモリの何かを変えたようだが、大した変化なし プロセス数が大きくなったら効果あるかも\n",
    "import multiprocessing\n",
    "from itertools import islice\n",
    "\n",
    "def process_segment(start, end, letter_type, esperanto_to_x, x_to_jijofu, x_to_hat, replacements_1, replacements_2, shared_text):\n",
    "    # 指定された範囲のテキストを取得\n",
    "    segment = '\\n'.join(islice(shared_text, start, end))\n",
    "    segment = multiple_replace(segment, esperanto_to_x)\n",
    "    segment = multi_smart_replace(segment, replacements_1)\n",
    "    segment = multi_smart_replace(segment, replacements_2)\n",
    "\n",
    "    if letter_type == 1:\n",
    "        segment = multiple_replace(segment, x_to_jijofu)\n",
    "    elif letter_type == 2:\n",
    "        segment = multiple_replace(segment, x_to_hat)\n",
    "\n",
    "    return segment\n",
    "\n",
    "def parallel_process(text, num_processes, letter_type, esperanto_to_x, x_to_jijofu, x_to_hat, replacements_1, replacements_2):\n",
    "    # テキストを行で分割\n",
    "    lines = text.split('\\n')\n",
    "    num_lines = len(lines)\n",
    "\n",
    "    # マネージャを使用して共有メモリ上にテキストを配置\n",
    "    with multiprocessing.Manager() as manager:\n",
    "        shared_text = manager.list(lines)\n",
    "        tasks = []\n",
    "\n",
    "        for i in range(num_processes):\n",
    "            start = i * num_lines // num_processes\n",
    "            end = (i + 1) * num_lines // num_processes\n",
    "            tasks.append((start, end, letter_type, esperanto_to_x, x_to_jijofu, x_to_hat, replacements_1, replacements_2, shared_text))\n",
    "\n",
    "        with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "            results = pool.starmap(process_segment, tasks)\n",
    "\n",
    "    return '\\n'.join(result for result in results)\n",
    "# テキストの読み込み\n",
    "with open('例文(input).txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 置換辞書を準備\n",
    "replacements_1 = {key: sorted_vocab[key][0] for key in sorted_keys}\n",
    "replacements_2 = {sorted_vocab_copy[key][0]: key for key in sorted_keys if '&#' in sorted_vocab_copy[key][0]}\n",
    "\n",
    "# 並列処理の実行\n",
    "processed_text = parallel_process(text, 4, letter_type, esperanto_to_x, x_to_jijofu, x_to_hat, replacements_1, replacements_2)\n",
    "\n",
    "# 結果をファイルに書き込み\n",
    "with open('語根に対応する漢字によって置換された文章(output).html', 'w', encoding='utf-8') as f:\n",
    "    f.write(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38330ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
